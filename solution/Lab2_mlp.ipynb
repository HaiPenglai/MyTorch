{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5e5cdd",
   "metadata": {},
   "source": [
    "# Lab2:手写线性/激活/交叉熵算子+数字识别\n",
    "\n",
    "【视频演示：如何填写TODO通过测试】\n",
    "\n",
    "### 前置知识：\n",
    "知道矩阵、张量的概念，知道简单的python代码，一丢丢高中数学知识\n",
    "\n",
    "### 目标\n",
    "\n",
    "用四种角度理解矩阵乘法、激活函数和交叉熵损失设计的原理\n",
    "\n",
    "用自己手写的线性层、激活函数、交叉熵损失，训练出一个准确率大于95%的手写数字识别模型\n",
    "\n",
    "![](../.assets/custom_mlp_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda81ade",
   "metadata": {},
   "source": [
    "### 任务：理解并实现线性层\n",
    "\n",
    "问题：用自己的话描述一下最简单的神经网络要干什么？\n",
    "\n",
    "【有很多样本，每行是样本，每列是特征，例如房子面积、年龄、市区距离，要预测一些东西，例如租金、价格】\n",
    "\n",
    "问题：解决思路是？\n",
    "\n",
    "【每个样本输入神经网络，按照一定权重输出】\n",
    "\n",
    "![](../.assets/multi_sample_linear.jpg)\n",
    "\n",
    "问题：经典的神经网络图（神经元和连接）和线性层有什么关系？\n",
    "\n",
    "【等价关系】\n",
    "\n",
    "问题：为什么行向量乘以矩阵等价于神经网络连接？\n",
    "\n",
    "【行向量和W第一列算出第一个神经元输出就理解了】\n",
    "\n",
    "![](../.assets/matmul_and_connect.jpg)\n",
    "\n",
    "问题：如何用四种方式对矩阵乘法进行理解？\n",
    "\n",
    "【线性层、Transformer注意力、加权平均、LLMINT8外积分解】\n",
    "\n",
    "![](../.assets/how_to_understand_mat_mul.jpg)\n",
    "\n",
    "问题：线性层这种矩阵乘法的理解为什么成立？\n",
    "\n",
    "【因为矩阵分块乘法的性质】\n",
    "\n",
    "问题：多个样本输入，从矩阵视角和神经网络视角如何理解\n",
    "\n",
    "【矩阵视角就是分别矩阵乘法然后拼接，神经网络视角就是多个样本输入神经网络】\n",
    "\n",
    "问题：加偏置如何理解？\n",
    "\n",
    "【只有一条线的连接】\n",
    "\n",
    "问题：为什么pytorch当中都是激活在前？\n",
    "\n",
    "【因为人习惯从左到右，可以连续的((XW)W)，但是如果W在前就不舒服】\n",
    "\n",
    "问题：如果X是更高维度，如何理解线性层？\n",
    "\n",
    "【最后一个维度看作样本，单独线性，最后按照原来的形状拼接】\n",
    "\n",
    "问题：为什么pytorch当中把X的列数量叫做input_features, 输出的列数量叫做output_features？\n",
    "\n",
    "【因为代表输入输出样本的特征数】\n",
    "\n",
    "问题：input_features和output_features和输入输出神经元的个数有什么关系？\n",
    "\n",
    "【相等的关系】\n",
    "\n",
    "问题：把一部分w设为0为什么叫做剪枝？\n",
    "\n",
    "【因为w是边】\n",
    "\n",
    "问题：使用X @ W.T 是否需要复制W.T\n",
    "\n",
    "【不需要，W.T返回视图而不是拷贝，修改了数据的理解方式】\n",
    "\n",
    "问题：pytorch定义属性的时候，为什么要用nn.Parameter()\n",
    "\n",
    "【否则不被视为可训练参数，不会放入state_dict，而测试中需要将标准模型的state_dict加载到自定义实现】\n",
    "\n",
    "检查一下`pip show mytorch`是否是自己实现的mytorch，完成TODO\n",
    "\n",
    "TODO：实现`mytorch\\mytorch\\mynn\\mlp\\linear.py`当中的三个TODO，其中初始化使用`nn.init.kaiming_uniform_(self.weight)`，详细推导见之后"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题，什么是state_dict, 测试的逻辑是什么？\n",
    "# 【state_dict记录了模型权重名称和具体权重。测试的逻辑是，尝试把官方模型加载到自己实现的模型，然后比较推理结果】\n",
    "\n",
    "import torch\n",
    "linaer_layer = torch.nn.Linear(3, 4)\n",
    "for key, param in linaer_layer.state_dict().items():\n",
    "    print(key, param.shape)\n",
    "    \n",
    "# 问题：state_dict的名称是如何确定的？\n",
    "# 【模型的层的属性名嵌套确定的，假如模型model.layer1.layer2.weight可以访问一个权重，那么名称就是layer1.layer2.weight】\n",
    "\n",
    "class SubModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(2, 3)  # 子模块\n",
    "\n",
    "class MainModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = SubModel()  # 嵌套子模型\n",
    "        self.layer2 = torch.nn.Linear(3, 1)\n",
    "\n",
    "model = MainModel()\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8291e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置图像风格\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Activation Functions and Their Derivatives', fontsize=16)\n",
    "\n",
    "# 定义输入范围\n",
    "x = np.linspace(-5, 5, 500)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. ReLU 及其导数\n",
    "# ------------------------------\n",
    "relu = np.maximum(0, x)\n",
    "relu_grad = (x > 0).astype(float)\n",
    "\n",
    "axes[0, 0].plot(x, relu, label='ReLU', color='blue')\n",
    "axes[0, 0].set_title('ReLU')\n",
    "axes[1, 0].plot(x, relu_grad, label='ReLU Derivative', color='red')\n",
    "axes[1, 0].set_title('ReLU Derivative')\n",
    "\n",
    "# ------------------------------\n",
    "# 2. LeakyReLU 及其导数 (alpha=0.1)\n",
    "# ------------------------------\n",
    "alpha = 0.1\n",
    "leaky_relu = np.where(x > 0, x, alpha * x)\n",
    "leaky_relu_grad = np.where(x > 0, 1, alpha)\n",
    "\n",
    "axes[0, 1].plot(x, leaky_relu, label='LeakyReLU', color='green')\n",
    "axes[0, 1].set_title(f'LeakyReLU (α={alpha})')\n",
    "axes[1, 1].plot(x, leaky_relu_grad, label='LeakyReLU Derivative', color='purple')\n",
    "axes[1, 1].set_title('LeakyReLU Derivative')\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Sigmoid 及其导数\n",
    "# ------------------------------\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "sigmoid_grad = sigmoid * (1 - sigmoid)\n",
    "\n",
    "axes[0, 2].plot(x, sigmoid, label='Sigmoid', color='orange')\n",
    "axes[0, 2].set_title('Sigmoid')\n",
    "axes[1, 2].plot(x, sigmoid_grad, label='Sigmoid Derivative', color='brown')\n",
    "axes[1, 2].set_title('Sigmoid Derivative')\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Tanh 及其导数\n",
    "# ------------------------------\n",
    "tanh = np.tanh(x)\n",
    "tanh_grad = 1 - tanh ** 2\n",
    "\n",
    "axes[0, 3].plot(x, tanh, label='Tanh', color='magenta')\n",
    "axes[0, 3].set_title('Tanh')\n",
    "axes[1, 3].plot(x, tanh_grad, label='Tanh Derivative', color='cyan')\n",
    "axes[1, 3].set_title('Tanh Derivative')\n",
    "\n",
    "# 统一设置坐标轴和网格\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        axes[i, j].grid(True, linestyle='--', alpha=0.6)\n",
    "        axes[i, j].axhline(0, color='black', linewidth=0.5)\n",
    "        axes[i, j].axvline(0, color='black', linewidth=0.5)\n",
    "        axes[i, j].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b813edf",
   "metadata": {},
   "source": [
    "### 任务：理解并手动实现激活函数\n",
    "\n",
    "问题：只有线性层可以吗，所以呢\n",
    "\n",
    "【不行，因为可以把W合并，所以要有非线性函数】\n",
    "\n",
    "问题：为什么把X叫做激活值\n",
    "\n",
    "【因为上一层神经网络输出作为下一层的输入，上层的输出来自于激活】\n",
    "\n",
    "问题：现在还用relu吗\n",
    "\n",
    "【有更好的选择，例如gelu】\n",
    "\n",
    "---\n",
    "\n",
    "**1. ReLU (Rectified Linear Unit)**\n",
    "• 函数：\n",
    "\n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "• 导数：\n",
    "\n",
    "  $$\n",
    "  f'(x) = \\begin{cases} \n",
    "  1 & \\text{if } x > 0 \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "• 特点：\n",
    "\n",
    "  • 计算简单，缓解梯度消失问题（正区间梯度为 1）。\n",
    "\n",
    "  • 缺点：负区间梯度为 0，可能导致“神经元死亡”。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**2. LeakyReLU (Leaky Rectified Linear Unit)**\n",
    "• 函数（$\\alpha$ 为小常数，如 0.1）：\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} \n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha x & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "• 导数：\n",
    "\n",
    "  $$\n",
    "  f'(x) = \\begin{cases} \n",
    "  1 & \\text{if } x > 0 \\\\\n",
    "  \\alpha & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "• 特点：\n",
    "\n",
    "  • 解决 ReLU 的“神经元死亡”问题（负区间梯度为 $\\alpha$）。\n",
    "\n",
    "  • $\\alpha$ 通常设为 0.01 或 0.1。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3. Sigmoid**\n",
    "• 函数：\n",
    "\n",
    "  $$\n",
    "  f(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "• 导数：\n",
    "\n",
    "  $$\n",
    "  f'(x) = f(x) \\cdot (1 - f(x))\n",
    "  $$\n",
    "• 特点：\n",
    "\n",
    "  • 输出范围 $[0, 1]$，适合概率输出。\n",
    "\n",
    "  • 缺点：梯度消失问题严重（当 $x \\to \\pm\\infty$ 时，导数接近 0）。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**4. Tanh (Hyperbolic Tangent)**\n",
    "• 函数：\n",
    "\n",
    "  $$\n",
    "  f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "• 导数：\n",
    "\n",
    "  $$\n",
    "  f'(x) = 1 - \\tanh^2(x)\n",
    "  $$\n",
    "• 特点：\n",
    "\n",
    "  • 输出范围 $[-1, 1]$，均值为 0，比 Sigmoid 更适合隐藏层。\n",
    "\n",
    "  • 梯度消失问题仍存在（但比 Sigmoid 轻微）。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**5. Softmax**\n",
    "• 函数（多分类输出层）：\n",
    "\n",
    "  $$\n",
    "  f(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "  $$\n",
    "  • 其中 $K$ 是类别数，$z_i$ 是第 $i$ 个类别的 logits。\n",
    "\n",
    "• 导数（对 $z_i$ 和 $z_j$ 的偏导）：\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f(z_i)}{\\partial z_j} = \\begin{cases} \n",
    "  f(z_i) \\cdot (1 - f(z_j)) & \\text{if } i = j \\\\\n",
    "  -f(z_i) \\cdot f(z_j) & \\text{if } i \\neq j\n",
    "  \\end{cases}\n",
    "  $$\n",
    "• 特点：\n",
    "\n",
    "  • 输出为概率分布（所有类别和为 1）。\n",
    "\n",
    "  • 仅用于多分类输出层，不适用于隐藏层。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70a29b",
   "metadata": {},
   "source": [
    "问题： 如何记住上面的激活函数公式？导数公式？\n",
    "\n",
    "【答案：见的次数太多了很好记。导数不需要记住，需要的时候推导即可。】\n",
    "\n",
    "问题：如何手动实现自定义梯度计算？复杂神经网络的梯度计算是如何进行的？\n",
    "\n",
    "【答案：自定义backward函数，放在二值量化当中讲。复杂函数梯度由梯度算子实现计算】\n",
    "\n",
    "问题：如何理解tanh和sigmoid\n",
    "\n",
    "【tanh类似于范围扩大一倍的sigmoid】\n",
    "\n",
    "问题：tanh的单调性如何理解，为什么叫做tanh\n",
    "\n",
    "【负无穷大的时候，e^-x占主导，后面同理。因为具有sinh、cosh、tanh和sin、cos、tan有类似的公式，因为sinh是二分之分子，sinh奇函数，cosh偶函数】\n",
    "\n",
    "问题：实现的softmax的步骤\n",
    "\n",
    "【求e^, 求和，除以】\n",
    "\n",
    "问题：为什么softmax可以表示概率\n",
    "\n",
    "【答案：1.数越大softmax越大，2.总和为1】\n",
    "\n",
    "问题：为什么softmax原始公式有点不稳定，要如何改进\n",
    "\n",
    "【因为万一有一个数很大，可以通过除以e^最大数解决，此时分子为1，此时最大数为1，其他数为0】\n",
    "\n",
    "问题：上述改进等价于什么\n",
    "\n",
    "【等价于一开始所有数就减去最大值】\n",
    "\n",
    "问题：张量如何求激活函数\n",
    "\n",
    "【逐元素做，softmax需要考虑维度】\n",
    "\n",
    "问题：有线性、偏置、激活的神经网络可视化是怎么样的\n",
    "\n",
    "【之前可视化，但是输出作为输入】\n",
    "\n",
    "TODO: 完成`mytorch\\mytorch\\mynn\\mlp`当中的relu、sigmoid、softmax、tanh算子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b662a",
   "metadata": {},
   "source": [
    "### 任务：理解并手动实现交叉熵损失\n",
    "\n",
    "![](../.assets/how_to_understand_celoss.jpg)\n",
    "\n",
    "问题：交叉熵损失为什么要这么设计\n",
    "\n",
    "【预测越准确损失越小】\n",
    "\n",
    "问题：交叉熵不会是负无穷吗\n",
    "\n",
    "【softmax数值是稳定的，概率即使是10^-50，其实-log也不是很大】\n",
    "\n",
    "TODO：完成`mytorch\\mytorch\\mynn\\loss\\`当中的交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b176a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义函数\n",
    "def neg_log(x):\n",
    "    return -np.log(x)  # 默认 np.log 是自然对数（ln）\n",
    "\n",
    "# 生成 x 值（避免 x=0 和 x<0）\n",
    "x = np.linspace(0.001, 3, 500)  # 从接近 0 开始，避免 log(0) 无定义\n",
    "y = neg_log(x)\n",
    "\n",
    "# 绘制图像\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label=r\"$y = -\\ln(x)$\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# 设置坐标轴和标题\n",
    "plt.title(r\"Plot of $y = -\\ln(x)$ on $[0, 3]$\", fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=12)\n",
    "plt.ylabel(\"y\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# 处理 x=0 的渐近行为\n",
    "plt.axvline(x=0, color=\"red\", linestyle=\"--\", label=\"Vertical asymptote (x=0)\")\n",
    "plt.ylim(-2, 5)  # 限制 y 轴范围，避免图像过于陡峭\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f54a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 3x4 的示例张量\n",
    "tensor_2d = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "print(\"原始张量:\\n\", tensor_2d)\n",
    "\n",
    "row_index = torch.tensor([0,2])\n",
    "col_index = torch.tensor([1, 3])\n",
    "print(tensor_2d[row_index, col_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24bdc8",
   "metadata": {},
   "source": [
    "### 任务：用自己实现的线性层、交叉熵、训练模型\n",
    "\n",
    "问题：训练简单模型的基本流程是什么\n",
    "\n",
    "【很经典，不赘述】\n",
    "\n",
    "TODO：完成`model\\mnist\\mlp`当中的代码，分别使用标准实现和自己的实现训练模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
